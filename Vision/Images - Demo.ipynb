{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n\n## Manipulating Images\n\n\n### Load an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.color as sc\n",
    "\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg -o img.jpg\n",
    "\n",
    "i = np.array(Image.open('img.jpg'))\n",
    "imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Numerical Properties of the Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "type(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i_mono = sc.rgb2gray(i)\n",
    "imshow(i_mono, cmap='gray')\n",
    "i_mono.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Pixel Value Distributions\nPlot a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def im_hist(img):\n",
    "    import matplotlib.pyplot as plt    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()    \n",
    "    ax.hist(img.flatten(), bins = 256)\n",
    "    plt.show()\n",
    "\n",
    "im_hist(i_mono)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a cumulative histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def im_cdf(img):\n",
    "    import matplotlib.pyplot as plt    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()    \n",
    "    ax.hist(img.flatten(), bins = 256, cumulative=True)\n",
    "    plt.show()\n",
    "    \n",
    "im_cdf(i_mono)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize the Image\nUse skimage library to equalize the image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "i_eq = exposure.equalize_hist(i_mono)\n",
    "imshow(i_eq, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the histogram and CDF plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "im_hist(i_eq)\n",
    "im_cdf(i_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising with Filters\n\n### Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "i_n = skimage.util.random_noise(i_eq)\n",
    "imshow(i_n, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Gaussian Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gauss_filter(im, sigma = 10):\n",
    "    from scipy.ndimage.filters import gaussian_filter as gf\n",
    "    import numpy as np\n",
    "    return gf(im, sigma = sigma)   \n",
    "i_g = gauss_filter(i_n)\n",
    "imshow(i_g, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Median Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def med_filter(im, size = 10):\n",
    "    from scipy.ndimage.filters import median_filter as mf\n",
    "    import numpy as np\n",
    "    return mf(im, size = size)     \n",
    "i_m = med_filter(i_n)\n",
    "imshow(i_m, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features\n\n#### Sobel Edge Detection\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def edge_sobel(image):\n",
    "    from scipy import ndimage\n",
    "    import skimage.color as sc\n",
    "    import numpy as np\n",
    "    image = sc.rgb2gray(image) # Convert color image to gray scale\n",
    "    dx = ndimage.sobel(image, 1)  # horizontal derivative\n",
    "    dy = ndimage.sobel(image, 0)  # vertical derivative\n",
    "    mag = np.hypot(dx, dy)  # magnitude\n",
    "    mag *= 255.0 / np.amax(mag)  # normalize (Q&D)\n",
    "    mag = mag.astype(np.uint8)\n",
    "    return mag\n",
    "\n",
    "i_edge = edge_sobel(i_m)\n",
    "imshow(i_edge, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harris Corner Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def corner_harr(im, min_distance = 10):\n",
    "    from skimage.feature import corner_harris, corner_peaks\n",
    "    mag = corner_harris(im)\n",
    "    return corner_peaks(mag, min_distance = min_distance)\n",
    "\n",
    "harris = corner_harr(i_eq, 10)\n",
    "\n",
    "\n",
    "def plot_harris(im, harris, markersize = 20, color = 'red'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()    \n",
    "    ax.imshow(np.array(im).astype(float), cmap=\"gray\")\n",
    "    ax.plot(harris[:, 1], harris[:, 0], 'r+', color = color, markersize=markersize)\n",
    "    return 'Done'  \n",
    "\n",
    "plot_harris(i_eq, harris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corner detection algorithm has identified the eyes in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n\n### Custom Vision API\nhttps://www.customvision.ai/projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the Custom Vision SDK\n",
    "! pip install azure-cognitiveservices-vision-customvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PREDICTION_KEY = 'YOUR_PREDICTION_KEY'\n",
    "ENDPOINT='https://YOUR_REGION.api.cognitive.microsoft.com'\n",
    "PROJECT_ID = 'YOUR_PROJECT_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "%matplotlib inline\n",
    "\n",
    "# Use two test images\n",
    "test_img1_url = 'http://www.pachd.com/free-images/food-images/apple-01.jpg'\n",
    "test_img2_url = 'http://www.pachd.com/free-images/food-images/carrot-02.jpg'\n",
    "\n",
    "# Create an instance of the prediction service\n",
    "predictor = CustomVisionPredictionClient(PREDICTION_KEY, endpoint=ENDPOINT)\n",
    "\n",
    "\n",
    "# Get a prediction for image 1\n",
    "result1 = predictor.predict_image_url(PROJECT_ID, url=test_img1_url)\n",
    "# The results include a prediction for each tag, in descending order of probability - so we'll get the first one\n",
    "prediction1 = result1.predictions[0].tag_name + \": {0:.2f}%\".format(result1.predictions[0].probability * 100)\n",
    "\n",
    "# Get a prediction for image 2\n",
    "result2 = predictor.predict_image_url(PROJECT_ID, url=test_img2_url)\n",
    "prediction2 = result2.predictions[0].tag_name + \": {0:.2f}%\".format(result2.predictions[0].probability * 100)\n",
    "    \n",
    "# Download the images so we can show them\n",
    "response = requests.get(test_img1_url)\n",
    "img1 = Image.open(BytesIO(response.content))\n",
    "\n",
    "response = requests.get(test_img2_url)\n",
    "img2 = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Subplot for first image and its predicted class\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(img1)\n",
    "a.set_title(prediction1)\n",
    "\n",
    "# Subplot for second image and its predicted class\n",
    "a=fig.add_subplot(1,2,2)\n",
    "imgplot = plt.imshow(img2)\n",
    "a.set_title(prediction2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use the Computer Vision API\nhttps://portal.azure.com.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visionURI = 'YOUR_REGION.api.cognitive.microsoft.com'\n",
    "visionKey = 'YOUR_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get An Image from a URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\n",
    "\n",
    "# Get the image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Computer Vision API to Get Image Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_features(img_url):\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "    headers = {\n",
    "        # Request headers.\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': visionKey,\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters. All of them are optional.\n",
    "        'visualFeatures': 'Categories,Description,Color',\n",
    "        'language': 'en',\n",
    "    })\n",
    "\n",
    "    body = \"{'url':'\" + img_url + \"'}\"\n",
    "\n",
    "    try:\n",
    "        # Execute the REST API call and get the response.\n",
    "        conn = http.client.HTTPSConnection(visionURI)\n",
    "        conn.request(\"POST\", \"/vision/v1.0/analyze?%s\" % params, body, headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "        # 'data' contains the JSON response.\n",
    "        parsed = json.loads(data)\n",
    "        if response is not None:\n",
    "            return parsed\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:')\n",
    "        print(e)\n",
    "        \n",
    "jsonData = get_image_features(img_url)\n",
    "desc = jsonData['description']['captions'][0]['text']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the full response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# View the full details returned\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "print (json.dumps(jsonData, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a different image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/soccer.jpg'\n",
    "\n",
    "# Get the image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n",
    "jsonData = get_image_features(img_url)\n",
    "desc = jsonData['description']['captions'][0]['text']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Face API\n\n\n### Create a Face API Service\nhttps://portal.azure.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "faceURI = \"https://YOUR_REGION.api.cognitive.microsoft.com/face/v1.0/\"\n",
    "faceKey = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Face SDK package. This makes it easier to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cognitive_face\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect a face in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageDraw\n",
    "import cognitive_face as CF\n",
    "\n",
    "# Set URI and Key\n",
    "CF.Key.set(faceKey)\n",
    "CF.BaseUrl.set(faceURI)\n",
    "\n",
    "# Detect faces in an image\n",
    "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme1.jpg'\n",
    "result = CF.face.detect(img_url)\n",
    "\n",
    "# Get the ID of the first face detected\n",
    "face1 = result[0]['faceId']\n",
    "print (\"Face 1:\" + face1)\n",
    "\n",
    "# Get the image\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Add rectangles for each face found\n",
    "color=\"blue\"\n",
    "if result is not None:\n",
    "    draw = ImageDraw.Draw(img) \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        left = faceRectangle['left']\n",
    "        top = faceRectangle['top']\n",
    "        width = faceRectangle['width']\n",
    "        height = faceRectangle['height']\n",
    "        draw.line([(left,top),(left+width,top)],fill=color, width=5)\n",
    "        draw.line([(left+width,top),(left+width,top+height)],fill=color , width=5)\n",
    "        draw.line([(left+width,top+height),(left, top+height)],fill=color , width=5)\n",
    "        draw.line([(left,top+height),(left, top)],fill=color , width=5)\n",
    "\n",
    "# show the image\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the image to compare\n",
    "img2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\n",
    "response2 = requests.get(img2_url)\n",
    "img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "# Detect faces in a comparison image\n",
    "result2 = CF.face.detect(img2_url)\n",
    "\n",
    "# Assume the first face is the one we want to compare\n",
    "face2 = result2[0]['faceId']\n",
    "print (\"Face 2:\" + face2)\n",
    "\n",
    "def verify_face(face1, face2):\n",
    "    # By default, assume the match is unverified\n",
    "    verified = \"Not Verified\"\n",
    "    color=\"red\"\n",
    "\n",
    "    if result2 is not None:\n",
    "        # compare the comparison face to the original one we retrieved previously\n",
    "        verify = CF.face.verify(face1, face2)\n",
    "\n",
    "        # if there's a match, set verified and change color to green\n",
    "        if verify['isIdentical'] == True:\n",
    "            verified = \"Verified\"\n",
    "            color=\"lightgreen\"\n",
    "\n",
    "        # Display the second face with a red rectange if unverified, or green if verified\n",
    "        draw = ImageDraw.Draw(img2) \n",
    "        for currFace in result2:\n",
    "            faceRectangle = currFace['faceRectangle']\n",
    "            left = faceRectangle['left']\n",
    "            top = faceRectangle['top']\n",
    "            width = faceRectangle['width']\n",
    "            height = faceRectangle['height']\n",
    "            draw.line([(left,top),(left+width,top)] , fill=color, width=5)\n",
    "            draw.line([(left+width,top),(left+width,top+height)] , fill=color, width=5)\n",
    "            draw.line([(left+width,top+height),(left, top+height)] , fill=color, width=5)\n",
    "            draw.line([(left,top+height),(left, top)] , fill=color, width=5)\n",
    "\n",
    "    # show the image\n",
    "    imshow(img2)\n",
    "\n",
    "    # Display verification status and confidence level\n",
    "    print(verified)\n",
    "    print (\"Confidence Level: \" + str(verify['confidence']))\n",
    "\n",
    "verify_face(face1, face2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the image to compare\n",
    "img2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme3.jpg'\n",
    "response2 = requests.get(img2_url)\n",
    "img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "# Detect faces in a comparison image\n",
    "result2 = CF.face.detect(img2_url)\n",
    "\n",
    "# Assume the first face is the one we want to compare\n",
    "face2 = result2[0]['faceId']\n",
    "print (\"Face 2:\" + face2)\n",
    "\n",
    "verify_face(face1, face2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the image to compare\n",
    "img2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/satya.jpg'\n",
    "response2 = requests.get(img2_url)\n",
    "img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "# Detect faces in a comparison image\n",
    "result2 = CF.face.detect(img2_url)\n",
    "\n",
    "# Assume the first face is the one we want to compare\n",
    "face2 = result2[0]['faceId']\n",
    "print (\"Face 2:\" + face2)\n",
    "\n",
    "verify_face(face1, face2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No match!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
